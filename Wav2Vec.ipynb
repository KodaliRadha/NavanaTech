{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.11.3 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (4.11.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (0.0.53)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (5.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (4.12.0)\n",
      "Requirement already satisfied: requests in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (2.22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from transformers==4.11.3) (3.8.0)\n",
      "Requirement already satisfied: six in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers==4.11.3) (1.14.0)\n",
      "Requirement already satisfied: click in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers==4.11.3) (7.0)\n",
      "Requirement already satisfied: joblib in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers==4.11.3) (0.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.11.3) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.11.3) (4.3.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from requests->transformers==4.11.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from requests->transformers==4.11.3) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from requests->transformers==4.11.3) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from requests->transformers==4.11.3) (1.25.8)\n",
      "Requirement already satisfied: librosa in /home/vit-ap/anaconda3/lib/python3.7/site-packages (0.9.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (0.14.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (4.4.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (0.4.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (1.21.6)\n",
      "Requirement already satisfied: numba>=0.45.1 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (0.48.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (0.22.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (23.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (1.4.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa) (1.14.0)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from numba>=0.45.1->librosa) (0.31.0)\n",
      "Requirement already satisfied: setuptools in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from numba>=0.45.1->librosa) (45.2.0.post20200210)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from pooch>=1.0->librosa) (2.22.0)\n",
      "Requirement already satisfied: pycparser in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.19)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
      "Requirement already satisfied: jiwer in /home/vit-ap/anaconda3/lib/python3.7/site-packages (2.5.1)\n",
      "Requirement already satisfied: levenshtein==0.20.2 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from jiwer) (0.20.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from levenshtein==0.20.2->jiwer) (2.6.1)\n",
      "Requirement already satisfied: jarowinkler<2.0.0,>=1.2.0 in /home/vit-ap/anaconda3/lib/python3.7/site-packages (from rapidfuzz<3.0.0,>=2.3.0->levenshtein==0.20.2->jiwer) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets>=1.18.3\n",
    "!pip install transformers==4.11.3\n",
    "!pip install librosa\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/vit-ap/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForMaskedLM: ['project_hid.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForMaskedLM were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model = AutoModelWithLMHead.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Set the model to fine-tune mode\n",
    "model.train()\n",
    "\n",
    "# Define the path to your audio files and processed tensors\n",
    "path_audio = '/home/vit-ap/Desktop/Navana AI Intern/AudioRecordings'\n",
    "path_save = '/home/vit-ap/Desktop/Navana AI Intern/processedFiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, path_audio, path_save):\n",
    "        self.path_audio = path_audio\n",
    "        self.path_save = path_save\n",
    "        self.files = os.listdir(path_save)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        tensor = torch.load(os.path.join(self.path_save, file))\n",
    "        label = int(file.split('_')[0]) # Assumes file name format is 'label_filename.pt'\n",
    "        return tensor, label\n",
    "\n",
    "path_audio = '/home/vit-ap/Desktop/Navana AI Intern/AudioRecordings'\n",
    "path_save = '/home/vit-ap/Desktop/Navana AI Intern/processedFiles'\n",
    "dataset = AudioDataset(path_audio, path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForMaskedLM: ['project_hid.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForMaskedLM were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model = AutoModelWithLMHead.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Set the model to fine-tune mode\n",
    "model.train()\n",
    "\n",
    "# Define the path to your audio files and processed tensors\n",
    "path_audio = '/home/vit-ap/Desktop/Navana AI Intern/AudioRecordings'\n",
    "path_save = '/home/vit-ap/Desktop/Navana AI Intern/processedFiles'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "label_dict = {'positive': 1, 'negative': 0}\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, path_audio, path_save):\n",
    "        self.path_audio = path_audio\n",
    "        self.path_save = path_save\n",
    "        self.files = [f for f in os.listdir(path_save) if f.endswith('.pt')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        tensor = torch.load(os.path.join(self.path_save, file))\n",
    "        label = label_dict[file.split('_')[0]]\n",
    "        print(f'Tensor size: {tensor.size()}')\n",
    "        return tensor, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0fdf4528d2b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Tensor size: {tensor.size()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Tensor size: {tensor.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=40, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-abf46e5960d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Fine-tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define your optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(10):\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
